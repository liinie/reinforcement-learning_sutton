\documentclass{book}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\begin{document}

\include{dynamic_programming}
\include{monte_carlo_method}
\chapter{时序差分学习(Temporal-Difference Learning)}
如果我们要选出一个核心而新颖的强化学习的思想，它无疑是时序差分学习(TD)。
时序差分学习是蒙特卡罗方法和动态规划思想的结合。
类似蒙特卡罗方法，时序差分方法能够在没有环境动态模型的情况下直接通过原始经验学习。
像动态规划，时序差分法更新评估是部分基于其他学习到的评估，而不用等待最后的结果(他们用自助法(bootstrap))。
时序差分，动态规划与蒙特卡罗方法之间的关系是强化学习理论中一个不断重复出现的主题; 这一章是我们探索的开始。
在我们结束之前，我们会看到这些思想和方法会相互渗透并在很多方面相互结合。
特别地，在第七章我们介绍n步自助法(n-step bootstrapping),这个方法给时序差分和蒙特卡罗方法提供了一个桥梁，在第十二章我们会介绍TD($\lambda$) 算法，它可以把这两种算法天衣无缝地统一起来。

\section{时序差分预测}
时序差分和蒙特卡罗这两种方法都是用经验来解决预测问题。
已知一些经验遵从策略$\pi$，两种算法为在经验中经过的非终点状态$S_t$更新他们的估计值$V$ ($V$ of $v_\pi$)。
简单地说，蒙特卡罗方法要等到访问某一状态的回馈已知，才会把回馈作为$V(S_t)$的目标。
一个简单的适合非固定(nonstationary)环境的every-visit MC方法是：
\begin{align}
	V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)]
\end{align}

$G_t$是从时间$t$开始的实际回馈，$\alpha$是一个常量步长参数(constant step-size parameter)(参见[Equation 2.4])


\end{document}
